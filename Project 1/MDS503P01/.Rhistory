lines(cummin(mtcars$mpg), cummax(mtcars$mpg))
hist(mtcars$mpg)
lines(density(mtcars$mpg))
plot(density(mtcars$mpg))
hist(mtcars$mpg)
lines(density(mtcars$mpg))
lines(density(mtcars$mpg), lwd=3, lty=2)
hist(mtcars$mpg)
lines(density(mtcars$mpg), lwd=3, lty=2)
lines(plot(density(mtcars$mpg), lwd=3, lty=2))
hist(mtcars$mpg)
lines(plot(density(mtcars$mpg), lwd=3, lty=2))
hist(mtcars$mpg)
lines(density(table(mtcars$mpg)))
lines(density(table(mtcars$mpg)))
hist(density(mtcars$mpg))
hist(mtcars$mpg, probability = TRUE)
lines(density(mtcars$mpg))
barplot(table(mpg.bin))
data <- c(10, 15, 20, 25, 30, 35, 40, 45, 50)
cumulative_freq <- cumsum(table(data))
plot(cumulative_freq, type="s", main="Ogive of Data")
total_freq <- length(data)
median_freq <- (total_freq + 1)/2
median <- approx(cumulative_freq, 1:length(cumulative_freq), xout = median_freq)$y
median
abline(h=median, lty=2, col="red")
median(data)
median(data)
abline(h=median(data), lty=2, col="green")
abline(h=median(data), lty=2, col="green")
data <- c(10, 15, 20, 25, 30, 35, 40, 45, 50)
cumulative_freq <- cumsum(table(data))
plot(cumulative_freq, type="s", main="Ogive of Data")
total_freq <- length(data)
median_freq <- (total_freq + 1)/2
median <- approx(cumulative_freq, 1:length(cumulative_freq), xout = median_freq)$y
median
abline(h=median, lty=2, col="red")
abline(h=median(data), lty=2, col="green")
median(data)
total_freq
plot(cumulative_freq, type="s", main="Ogive of Data")
abline(h=median(data), lty=2, col="green")
abline(h=median, lty=2, col="red")
mode(mtcars$mpg)
hist(mtcars$mpg)
abline(v=mode(mtcars$mpg))
mpg.table <- table(mtcars$mpg)
mpg.table
#it has multiple high frequency so to calculate mode we use 3*median - 2*mean
mode <- 3*median(mtcars$mpg) - 2*mean(mtcars$mpg)
mode
abline(v=mode)
summary(mtcars$mpg)
plot(mtcars$mpg)
abline(v=mode)
hist(mtcars$mpg)
abline(v=mean, col="red")
?abline
abline(v=mean, lwd=3, col="red")
hist(mtcars$mpg)
abline(v=mean, lwd=3)
hist(mtcars$mpg)
abline(v=median(mtcars$mpg), lwd=3, lty=2)
abline(v=mean(mtcars$mpg), lwd=3)
abline(v=mean(mtcars$mpg), lwd=3, col="red")
abline(v=median(mtcars$mpg), lwd=3, col="green")
abline(v=mode, col="orange")
abline(v=mode, lwd=3 col="orange")
abline(v=mode, lwd=3, col="orange")
hist(mtcars$mpg, legends = ("topright", legend=c("mean", "median", "mode")))
hist(mtcars$mpg, legends("topright", legend=c("mean", "median", "mode")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange"), lwd=3))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange"), lty=1))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange"), lty=2))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
abline(v=mean(mtcars$mpg), lwd=3, col="red")
abline(v=median(mtcars$mpg), lwd=3, col="green")
abline(v=mode, lwd=3, col="orange")
hist(mtcars$mpg, col="blue" legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, col="blue", legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, bg="blue", legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
?hist
hist(mtcars$mpg, col = "blue", legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
abline(v=mean(mtcars$mpg), lwd=3, col="red")
abline(v=median(mtcars$mpg), lwd=3, col="green")
abline(v=mode, lwd=3, col="orange")
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
abline(v=mean(mtcars$mpg), lwd=3, col="red")
abline(v=median(mtcars$mpg), lwd=3, col="green")
abline(v=mode, lwd=3, col="orange")
hist(mtcars$mpg, legend("topright", legend=c("mean", "median", "mode"), fill=c("red", "green", "orange")))
#associated terms
library graph
library Rgraphviz
#associated terms
library("graph")
library("Rgraphviz")
#find most frequent used terms
frequentWord <- tm_map(myTdm, findMostFreqTerms(myTdm$dimnames$Terms))
setwd("D:/R programming runs/Class")
load(file = "rdmTweets.RData")
tweets <- rdmTweets
tweets
(n.tweet <- length(tweets))
strwrap(tweets[[154]]$text, width = 55)
strwrap(tweets[[154]]$text, width = 62)
tweets[1:3]
############ Loading twitterR & convert to data frame ########
library(twitteR)
#convert tweets to a data frame
df <- twListToDF(tweets)
str(df)
head(df)
library(tm)
#build a corpus
myCorpus <- Corpus(VectorSource(df$text))
#inspect first 3 elements
inspect(myCorpus[1:3])
#convert to lowercase
myCorpus <- tm_map(myCorpus, tolower)
inspect(myCorpus[1:3])
#remove punctuation and numbers
myCorpus <- tm_map(myCorpus, removePunctuation)
inspect(myCorpus[1:3])
myCorpus <- tm_map(myCorpus, removeNumbers)
inspect(myCorpus[1:3])
#remove URLs, http followed by non-space characters
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
inspect(myCorpus[1:3])
#remove r and big from the list of stopwords
myStopwords <- setdiff(stopwords("english"), c("r", "big"))
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
inspect(myCorpus[1:3])
######## Warning - be careful with stem #########
#keep a copy of corpus
myCorpusCopy <- myCorpus
#stem words  --- sets verb3, v4 to v1
myCorpus <- tm_map(myCorpus, stemDocument)
inspect(myCorpus[1:3])
#stem completion   - should show v1 to v4 from dictionary
#myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
inspect(myCorpus[1:3])
#text
myTdm <- TermDocumentMatrix(myCorpus)
str(myTdm)
#find most frequent used terms
frequentWord <- tm_map(myTdm, findMostFreqTerms(myTdm$dimnames$Terms))
?findFreqTerms
?findMostFreqTerms
freq <- findFreqTerms(pdf_tdm, lowfreq=10, highfreq=Inf)
freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
plot(myTdm, term=freq, corThreshold=0.7, weighting=T)
plot(myTdm, term=freq, corThreshold=0.1, weighting=T)
plot(myTdm, term=freq, corThreshold=0.1, weighting=T)
plot(myTdm, term=freq, corThreshold=2, weighting=T)
plot(myTdm, term=freq, corThreshold=0.05, weighting=T)
freq <- findMostFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
plot(myTdm, term=freq, corThreshold=0.05, weighting=T)
library(pdftools)
library(tm)
library(tidyverse)
library(wordcloud)
library(tidytext)
setwd("D:/R programming runs/Class")
kura <- list.files(pattern = "pdf$")
kuracorp <- Corpus(URISource(kura), readerControl=list(reader = readPDF()))
kuracorp
tdm <- TermDocumentMatrix(kuracorp, control = list(removePunctuation=T, stopwords=T, tolower=T, removeNumbers=T))
inspect(tdm)
kura.freq <- findFreqTerms(tdm, lowfreq=10, highfreq=Inf)
kura.freq
kura.matrix <- as.matrix(tdm[kura.freq,])
kura.matrix
hist(kura.matrix)
kura.sort <- sort(apply(kura.matrix,1,sum), decreasing=T)
kura.sort
kura.df <- as.data.frame(kura.sort)
kura.df
kura.df <- cbind(rownames(kura.df), kura.df)
rownames(kura.df) <- NULL
colnames(kura.df) <- c("word", "count")
kura.df
hist(kura.df$count)
plot(kura.df$count, type = "h")
p.words <- c("yolmo", "kur", "body", "death", "culture", "nepal")
p.words
kura.tidy <- tidy(kuracorp)
kura.tidy
kura.data <- kura.tidy %>% select(-author, -datetimestamp, -description, -heading, -id, -language, -origin) %>% unnest_tokens(word, text) %>% mutate(position = row_number()) %>% filter(!word %in% tm::stopwords("en"))
kura.data
library(fuzzyjoin)
kura.prox <- kura.data %>% filter(word == "saturation") %>% select(focus_term = word, focus_position = position) %>% difference_inner_join(kura.data, by = c(focus_position = "position"), max_dist = 5) %>% mutate(distance = abs(focus_position - position))
kura.prox
kura.prox.group <- kura.prox %>% group_by(word) %>% summarize(number = n(), maximum_distance = max(distance), minimum_distance = min(distance), average_distance = mean(distance)) %>% arrange(desc(number))
kura.prox.group
kura.ana <- kura.prox.group %>% filter(word %>% p.words)
library("graph")
library("Rgraphviz")
plot(tdm, term=kura.freq, corThreshold=0.05, weighting=T)
plot(tdm, term=kura.freq, corThreshold=0.1, weighting=T)
plot(tdm, term=kura.freq, corThreshold=1, weighting=T)
plot(tdm, term=kura.freq, corThreshold=0.1, weighting=T)
plot(tdm, term=kura.freq, corThreshold=0.001, weighting=T)
myTdm <- TermDocumentMatrix(kuracorp)
plot(myTdm, term=kura.freq, corThreshold=0.001, weighting=T)
plot(myTdm, term=kura.freq, corThreshold=0.1, weighting=T)
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
plot(myTdm, term=kura.freq, corThreshold=0.1, weighting=T)
plot(myTdm, term=kura.freq, corThreshold=1, weighting=T)
plot(myTdm, term=kura.freq, corThreshold=1, weighting=T)
plot(myTdm, term=kura.freq, corThreshold=1, weighting=F)
plot(myTdm, term=kura.freq, corThreshold=0.1, weighting=F)
plot(myTdm, term=kura.freq, corThreshold=0.01, weighting=F)
plot(myTdm, term=kura.freq, corThreshold=0.001, weighting=F)
plot(myTdm, term=kura.freq, corThreshold=0.0001, weighting=F)
plot(myTdm, term=kura.freq, corThreshold=0.00001, weighting=F)
tdm <- TermDocumentMatrix(kuracorp)
tdm <- removeSparseTerms(tdm, 0.5)
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
plot(tdm, term=kura.freq, corThreshold=0.00001, weighting=F)
plot(tdm, term=kura.freq, corThreshold=0.1, weighting=F)
plot(tdm, term=kura.freq, corThreshold=3, weighting=F)
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(graph)
library(Rgraphviz)
setwd('D:/R programming runs/Assisgnments/Project 1/MDS503P01')
pdf_files <- list.files(pattern = "pdf$")
pdf_corp <- Corpus(URISource(pdf_files), readerControl=list(reader = readPDF()))
pdf_corp
#pdf_tdm <- TermDocumentMatrix(pdf_corp, control = list(removePunctuation=T, stopwords=T, tolower=T, removeNumbers=T))
#inspect(pdf_tdm)
pdf_corp <- tm_map(pdf_corp, removePunctuation)
pdf_corp <- tm_map(pdf_corp, content_transformer(tolower))
pdf_corp <- tm_map(pdf_corp, removeWords, stopwords("english"))
inspect(pdf_corp)
pdf_tdm <- TermDocumentMatrix(pdf_corp)
#pdf_tdm <- removeSparseTerms(pdf_tdm, 0.5)
pdf.freq <- findFreqTerms(pdf_tdm, lowfreq=10, highfreq=Inf)
pdf.freq
pdf.matrix <- as.matrix(pdf_tdm[pdf.freq,])
pdf.matrix
hist(pdf.matrix)
pdf.sort <- sort(apply(pdf.matrix,1,sum), decreasing=T)
pdf.sort
pdf.df <- as.data.frame(pdf.sort)
pdf.df
# plot word cloud
m <- as.matrix(pdf_tdm)
freq <- sort(rowSums(m), decreasing=T)
wc <- wordcloud(words=names(freq), freq=freq, min.freq=4,
random.order=F)
wc
my_colors <- brewer.pal(8, "Paired")
wc_col <- wordcloud(words=names(freq), freq=freq, min.freq=4,
random.order=F, colors = my_colors)
wc_col
plot(pdf_tdm, term=pdf.freq, corThreshold=7, weighting=T)
library(pdftools)
library(tm)
library(tidyverse)
library(wordcloud)
library(tidytext)
setwd("D:/R programming runs/Class")
kura <- list.files(pattern = "pdf$")
kuracorp <- Corpus(URISource(kura), readerControl=list(reader = readPDF()))
kuracorp
tdm <- TermDocumentMatrix(kuracorp, control = list(removePunctuation=T, stopwords=T, tolower=T, removeNumbers=T))
inspect(tdm)
kura.freq <- findFreqTerms(tdm, lowfreq=10, highfreq=Inf)
kura.freq
kura.matrix <- as.matrix(tdm[kura.freq,])
kura.matrix
hist(kura.matrix)
kura.sort <- sort(apply(kura.matrix,1,sum), decreasing=T)
kura.sort
kura.df <- as.data.frame(kura.sort)
kura.df
kura.df <- cbind(rownames(kura.df), kura.df)
rownames(kura.df) <- NULL
colnames(kura.df) <- c("word", "count")
kura.df
hist(kura.df$count)
plot(kura.df$count, type = "h")
p.words <- c("yolmo", "kur", "body", "death", "culture", "nepal")
p.words
kura.tidy <- tidy(kuracorp)
kura.tidy
kura.data <- kura.tidy %>% select(-author, -datetimestamp, -description, -heading, -id, -language, -origin) %>% unnest_tokens(word, text) %>% mutate(position = row_number()) %>% filter(!word %in% tm::stopwords("en"))
kura.data
library(fuzzyjoin)
kura.prox <- kura.data %>% filter(word == "saturation") %>% select(focus_term = word, focus_position = position) %>% difference_inner_join(kura.data, by = c(focus_position = "position"), max_dist = 5) %>% mutate(distance = abs(focus_position - position))
kura.prox
kura.prox.group <- kura.prox %>% group_by(word) %>% summarize(number = n(), maximum_distance = max(distance), minimum_distance = min(distance), average_distance = mean(distance)) %>% arrange(desc(number))
kura.prox.group
kura.ana <- kura.prox.group %>% filter(word %>% p.words)
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
library("graph")
library("Rgraphviz")
tdm <- TermDocumentMatrix(kuracorp)
tdm <- removeSparseTerms(tdm, 0.5)
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
plot(tdm, term=kura.freq, corThreshold=3, weighting="none")
plot(tdm, term=kura.freq, corThreshold=0.1, weighting="none")
kura.freq
kuracorp <- tm_map(kuracorp, removePunctuation)
kuracorp <- tm_map(kuracorp, content_transformer(tolower))
kuracorp <- tm_map(kuracorp, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(kuracorp)
tdm <- removeSparseTerms(tdm, 0.5)
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
kura.freq
plot(tdm, term=kura.freq, corThreshold=0.1, weighting="none")
plot(tdm, term=kura.freq, corThreshold=0.1, weighting="none")
plot(tdm, term=kura.freq, corThreshold=1, weighting="none")
pdf.freq <- findFreqTerms(pdf_tdm, lowfreq=10, highfreq=Inf)
pdf.freq
pdf.matrix <- as.matrix(pdf_tdm[pdf.freq,])
pdf.matrix
kuracorp <- tm_map(kuracorp, removePunctuation)
kuracorp <- tm_map(kuracorp, content_transformer(tolower))
kuracorp <- tm_map(kuracorp, removeWords, stopwords("english"))
tdm <- TermDocumentMatrix(kuracorp)
tdm <- removeSparseTerms(tdm, 0.5)
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
kura.freq
m <- as.matrix(tdm)
freq <- sort(rowSums(m), decreasing=T)
plot(m, term=freq, corThreshold=1, weighting="none")
setwd("D:/R programming runs/Class")
load(file = "rdmTweets.RData")
tweets <- rdmTweets
tweets
(n.tweet <- length(tweets))
strwrap(tweets[[154]]$text, width = 55)
strwrap(tweets[[154]]$text, width = 62)
tweets[1:3]
############ Loading twitterR & convert to data frame ########
library(twitteR)
#convert tweets to a data frame
df <- twListToDF(tweets)
str(df)
head(df)
library(tm)
#build a corpus
myCorpus <- Corpus(VectorSource(df$text))
#inspect first 3 elements
inspect(myCorpus[1:3])
#convert to lowercase
myCorpus <- tm_map(myCorpus, tolower)
inspect(myCorpus[1:3])
#remove punctuation and numbers
myCorpus <- tm_map(myCorpus, removePunctuation)
inspect(myCorpus[1:3])
myCorpus <- tm_map(myCorpus, removeNumbers)
inspect(myCorpus[1:3])
#remove URLs, http followed by non-space characters
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
inspect(myCorpus[1:3])
#remove r and big from the list of stopwords
myStopwords <- setdiff(stopwords("english"), c("r", "big"))
#remove stopwords
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
inspect(myCorpus[1:3])
######## Warning - be careful with stem #########
#keep a copy of corpus
myCorpusCopy <- myCorpus
#stem words  --- sets verb3, v4 to v1
myCorpus <- tm_map(myCorpus, stemDocument)
inspect(myCorpus[1:3])
#stem completion   - should show v1 to v4 from dictionary
#myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
inspect(myCorpus[1:3])
#text
myTdm <- TermDocumentMatrix(myCorpus)
str(myTdm)
#find most frequent used terms
#frequentWord <- tm_map(myTdm, findMostFreqTerms(myTdm$dimnames$Terms))
freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
#associated terms
library("graph")
library("Rgraphviz")
plot(myTdm, term=freq, corThreshold=0.05, weighting=T)
str(myTdm)
str(freq)
str(freq)
freq <- sort(rowSums(m), decreasing=T)
str(freq)
str(m)
str(tdm)
str(kura.freq)
kura.freq
kura.freq[-1]
kura.freq[-1]
kura.freq
kura.freq[-c(1,2)]
kura.freq <- kura.freq[-c(1,2)]
plot(tdm, term=kura.freq, corThreshold=1, weighting="none")
kura.freq <- kura.freq[-c(1,2)]
kura.freq
plot(tdm, term=kura.freq, corThreshold=1, weighting="none")
kura.freq <- findFreqTerms(myTdm, lowfreq=10, highfreq=Inf)
kura.freq
plot(tdm, term=kura.freq, corThreshold=1, weighting="none")
tdm <- TermDocumentMatrix(kuracorp)
tdm <- removeSparseTerms(tdm, 0.5)
kura.freq <- findFreqTerms(tdm, lowfreq=10, highfreq=Inf)
kura.freq
plot(tdm, term=kura.freq, corThreshold=1, weighting="none")
plot(tdm, term=kura.freq, corThreshold=0.1, weighting="none")
library(pdftools)
library(tm)
library(tidyverse)
library(wordcloud)
library(tidytext)
setwd("D:/R programming runs/Class")
kura <- list.files(pattern = "pdf$")
kuracorp <- Corpus(URISource(kura), readerControl=list(reader = readPDF()))
kuracorp
tdm <- TermDocumentMatrix(kuracorp, control = list(removePunctuation=T, stopwords=T, tolower=T, removeNumbers=T))
inspect(tdm)
kura.freq <- findFreqTerms(tdm, lowfreq=10, highfreq=Inf)
kura.freq
kura.matrix <- as.matrix(tdm[kura.freq,])
kura.matrix
hist(kura.matrix)
kura.sort <- sort(apply(kura.matrix,1,sum), decreasing=T)
kura.sort
kura.df <- as.data.frame(kura.sort)
kura.df
kura.df <- cbind(rownames(kura.df), kura.df)
rownames(kura.df) <- NULL
colnames(kura.df) <- c("word", "count")
kura.df
hist(kura.df$count)
plot(kura.df$count, type = "h")
p.words <- c("yolmo", "kur", "body", "death", "culture", "nepal")
p.words
kura.tidy <- tidy(kuracorp)
kura.tidy
kura.data <- kura.tidy %>% select(-author, -datetimestamp, -description, -heading, -id, -language, -origin) %>% unnest_tokens(word, text) %>% mutate(position = row_number()) %>% filter(!word %in% tm::stopwords("en"))
kura.data
library(fuzzyjoin)
kura.prox <- kura.data %>% filter(word == "saturation") %>% select(focus_term = word, focus_position = position) %>% difference_inner_join(kura.data, by = c(focus_position = "position"), max_dist = 5) %>% mutate(distance = abs(focus_position - position))
kura.prox
kura.prox.group <- kura.prox %>% group_by(word) %>% summarize(number = n(), maximum_distance = max(distance), minimum_distance = min(distance), average_distance = mean(distance)) %>% arrange(desc(number))
kura.prox.group
kura.ana <- kura.prox.group %>% filter(word %>% p.words)
library("graph")
library("Rgraphviz")
tdm <- TermDocumentMatrix(kuracorp)
tdm <- removeSparseTerms(tdm, 0.5)
kura.freq <- findFreqTerms(tdm, lowfreq=10, highfreq=Inf)
kura.freq
plot(tdm, term=kura.freq, corThreshold=0.1, weighting="none")
tdm
View(tdm)
View(tdm)
library(pdftools)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(graph)
library(Rgraphviz)
setwd('D:/R programming runs/Assisgnments/Project 1/MDS503P01')
pdf_files <- list.files(pattern = "pdf$")
pdf_corp <- Corpus(URISource(pdf_files), readerControl=list(reader = readPDF()))
pdf_corp
pdf_tdm <- TermDocumentMatrix(pdf_corp, control = list(removePunctuation=T, stopwords=T, tolower=T, removeNumbers=T))
inspect(pdf_tdm)
pdf.freq <- findFreqTerms(pdf_tdm, lowfreq=10, highfreq=Inf)
pdf.freq
pdf.matrix <- as.matrix(pdf_tdm[pdf.freq,])
pdf.matrix
hist(pdf.matrix)
pdf.sort <- sort(apply(pdf.matrix,1,sum), decreasing=T)
pdf.sort
pdf.df <- as.data.frame(pdf.sort)
pdf.df
# plot word cloud
m <- as.matrix(pdf_tdm)
freq <- sort(rowSums(m), decreasing=T)
wc <- wordcloud(words=names(freq), freq=freq, min.freq=4,
random.order=F)
wc
my_colors <- brewer.pal(8, "Paired")
wc_col <- wordcloud(words=names(freq), freq=freq, min.freq=4,
random.order=F, colors = my_colors)
wc_col
plot(pdf_tdm, term=pdf.freq, corThreshold=7, weighting=T)
# create a correlation matrix
cor_mat <- cor(m, method = "pearson")
# create a graph object
graph <- graph.adjacency(cor_mat, weighted = TRUE, mode = "undirected")
install.packages("C:/Users/user/Downloads/Compressed/Rgraphviz_1.18.1.tar.gz", repos = NULL, type = "source")
gc()
install.packages("C:/Users/user/Downloads/Compressed/Rgraphviz_1.18.1.tar.gz", repos = NULL, type = "source")
